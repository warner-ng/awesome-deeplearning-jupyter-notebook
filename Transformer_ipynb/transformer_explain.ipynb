{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"image/multi-head.png\" width=\"300\" alt=\"multi-head.png\">\n",
    "\n",
    "<img src=\"image/scale_dot_product_attention.png\" width=\"300\" alt=\"attention.png\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadedAttention(nn.Module):\n",
    "    def __init__(self, h, d_embed, Dropout=0.0): # 0% 概率dropout \n",
    "        super().__init__()\n",
    "        assert d_embed % h == 0 # 确认H头数整除维度数\n",
    "        self.d_k = d_embed // h # 每个头的输入维度\n",
    "        self.h = h \n",
    "        self.WQ = nn.linear(d_embed, d_embed) # 注意过的是d_embed，后面才分割\n",
    "        self.WK = nn.linear(d_embed, d_embed) # 注意过的是d_embed，后面才分割\n",
    "        self.WV = nn.linear(d_embed, d_embed) # 注意过的是d_embed，后面才分割\n",
    "        self.linear = nn.linear(d_embed, d_embed)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x_query, x_key, x_value, mask=None): # 训练的时候需要掩码，掩盖decoder未来的序列，防止decoder直接偷看未来的y造成训练作弊\n",
    "        nbatch = x_query.size(0) # x_query的第一维度就是numbers of batch\n",
    "        \"\"\"now we project it to multihead q k v\n",
    "        之前 x_query, x_key, x_value dimension: nbatch * seq_len * d_embed\n",
    "        处理后 query, key, value dimensions: nbatch * h * seq_len * d_k\n",
    "\n",
    "        \"\"\"\n",
    "        \n",
    "        query = self.WQ(x_query).view(nbatch, -1, self.h, self.d_k).transpose(1,2) \n",
    "        key   = self.WK(x_key).view(nbatch, -1, self.h, self.d_k).transpose(1,2)\n",
    "        value = self.WV(x_value).view(nbatch, -1, self.h, self.d_k).transpose(1,2)\n",
    "\n",
    "        # .view的作用是调整tensor的形状\n",
    "        # .transpose是改变tensor某两个维度的位置\n",
    "        # Q 与 K 与 V 的形状（输入）： (B, H, S, D_k)  batch head seq_len dimention_per_head\n",
    "        # Scores 的目标形状： (B, H, S, S) \n",
    "        # 所以要转置K的倒数第2 倒数第1 维度\n",
    "        scores = torch.matmul(query, key.transpose(-2, -1))/math.sqrt(self.d_k)\n",
    "\n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask, float('-inf'))\n",
    "\n",
    "        # p_atten dimensions: nbatch * h * seq_len * seq_len\n",
    "        p_atten = torch.nn.functional.softmax(scores, dim=-1)\n",
    "        p_atten = self.dropout(p_atten)\n",
    "\n",
    "        # x dimensions: nbatch * h * seq_len * d_k\n",
    "        x = torch.matmul(p_atten, value)\n",
    "\n",
    "        # x now has dimensions:nbtach * seq_len * d_embed\n",
    "        x = x.transpose(1, 2).contiguous().view(nbatch, -1, self.d_embed)\n",
    "        return self.linear(x) # final linear layer\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"image/transformer.png\" width=\"300\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualConnection(nn.Module):\n",
    "  '''residual connection: x + dropout(sublayer(layernorm(x))) '''\n",
    "  def __init__(self, dim, dropout):\n",
    "      super().__init__()\n",
    "      self.drop = nn.Dropout(dropout)\n",
    "      self.norm = nn.LayerNorm(dim)\n",
    "\n",
    "  def forward(self, x, sublayer): # sublayer之后在encoder   decoder里面会传进去\n",
    "      return x + self.drop(sublayer(self.norm(x))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    '''Encoder = token embedding + positional embedding -> a stack of N EncoderBlock -> layer norm'''\n",
    "    def __init__():\n",
    "        super().__init__()\n",
    "        self.d_embed = config.d_embed\n",
    "        self.tok_embed = nn.Embedding(config.encoder_vocab_size, config.d_embed) # embedding的定义是MLP\n",
    "        self.pos_embed = nn.Parameter(torch.zeros(1, config.max_seq_len, config.d_embed)) \n",
    "        # parameter定义是一组自己学习的参数，没有使用正弦余弦编码了这里\n",
    "        self.encoder_blocks = nn.ModuleList([EncoderBlock(config) for _ in range(config.N_encoder)])\n",
    "        # 创建很多个module实例\n",
    "        self.dropout = nn.Dropout(config.dropout)\n",
    "        self.norm = nn.LayerNorm(config.d_embed)\n",
    "\n",
    "\n",
    "    def forward (self, input, mask=None):\n",
    "        x = self.tok_embed(input)\n",
    "        x_pos = self.pos_embed[:, :x.size(1), :] \n",
    "        # 为了获得一个形状为 (1, 实际序列长度, config.d_embed) 的张量 x_pos\n",
    "        # 然后将它与输入张量 x 相加，从而为输入序列的每个词添加其位置信息\n",
    "        \n",
    "        x = self.dropout(x + x_pos) # 加上位置编码\n",
    "        for layer in self.encoder_blocks:\n",
    "            x = layer(x, mask)\n",
    "        return self.norm(x)\n",
    "\n",
    "\n",
    "class EncoderBlock(nn.Module):\n",
    "    '''EncoderBlock: self-attention -> position-wise fully connected feed-forward layer'''\n",
    "    def __init__(self, config):\n",
    "        super(EncoderBlock, self).__init__()\n",
    "        self.atten = MultiHeadedAttention(config.h, config.d_embed, config.dropout)\n",
    "        self.feed_forward = nn.Sequential(\n",
    "            nn.Linear(config.d_embed, config.d_ff),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(config.dropout),\n",
    "            nn.Linear(config.d_ff, config.d_embed)\n",
    "        )\n",
    "        self.residual1 = ResidualConnection(config.d_embed, config.dropout)\n",
    "        self.residual2 = ResidualConnection(config.d_embed, config.dropout)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        # self-attention\n",
    "        x = self.residual1(x, lambda x: self.atten(x, x, x, mask=mask))\n",
    "        # position-wise fully connected feed-forward layer\n",
    "        return self.residual2(x, self.feed_forward)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"image/transformer.png\" width=\"300\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    '''Decoder = token embedding + positional embedding -> a stack of N DecoderBlock -> fully-connected layer'''\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.d_embed = config.d_embed\n",
    "        self.tok_embed = nn.Embedding(config.decoder_vocab_size, config.d_embed)\n",
    "        self.pos_embed = nn.Parameter(torch.zeros(1, config.max_seq_len, config.d_embed))\n",
    "        self.dropout = nn.Dropout(config.dropout)\n",
    "        self.decoder_blocks = nn.ModuleList([DecoderBlock(config) for _ in range(config.N_decoder)])\n",
    "        self.norm = nn.LayerNorm(config.d_embed)\n",
    "        self.linear = nn.Linear(config.d_embed, config.decoder_vocab_size)\n",
    "\n",
    "\n",
    "    def future_mask(self, seq_len):\n",
    "        '''mask out tokens at future positions'''\n",
    "        mask = (torch.triu(torch.ones(seq_len, seq_len, requires_grad=False), diagonal=1)!=0).to(DEVICE)\n",
    "        # torch.ones 生成单位阵\n",
    "        # torch.triu 上三角矩阵\n",
    "        return mask.view(1, 1, seq_len, seq_len) # 拓展形状（广播，前面两个是B H）\n",
    "\n",
    "    def forward(self, memory, src_mask, trg, trg_pad_mask):\n",
    "        seq_len = trg.size(1)\n",
    "        trg_mask = torch.logical_or(trg_pad_mask, self.future_mask(seq_len)) # 未来的掩码\n",
    "        x = self.tok_embed(trg) + self.pos_embed[:, :trg.size(1), :]\n",
    "        x = self.dropout(x)\n",
    "        for layer in self.decoder_blocks:\n",
    "            x = layer(memory, src_mask, x, trg_mask)\n",
    "        x = self.norm(x)\n",
    "        logits = self.linear(x) # 先不过softmax\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderBlock(nn.Module):\n",
    "    ''' EncoderBlock: self-attention -> position-wise feed-forward (fully connected) layer'''\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.atten1 = MultiHeadedAttention(config.h, config.d_embed)\n",
    "        self.atten2 = MultiHeadedAttention(config.h, config.d_embed)\n",
    "        self.feed_forward = nn.Sequential(\n",
    "            nn.Linear(config.d_embed, config.d_ff),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(config.dropout),\n",
    "            nn.Linear(config.d_ff, config.d_embed)\n",
    "        )\n",
    "        self.residuals = nn.ModuleList([ResidualConnection(config.d_embed, config.dropout)\n",
    "                                       for i in range(3)])\n",
    "\n",
    "    def forward(self, memory, src_mask, decoder_layer_input, trg_mask):\n",
    "        x = memory # 传进来的 是 encoder 的输出，含有 k 和 v 信息的latent\n",
    "        y = decoder_layer_input\n",
    "        y = self.residuals[0](y, lambda y: self.atten1(y, y, y, mask=trg_mask))\n",
    "        # 上面是triangle掩码，就是因果掩码\n",
    "        # lambda表达式的作用式是一个local function，等到调用这个lambda的时候再进行lambda表达式的计算\n",
    "        # 也即是那个residual里面的sublayer\n",
    "        # 这里q k v 传的都是y 所以是self-attention\n",
    "        y = self.residuals[1](y, lambda y: self.atten2(y, x, x, mask=src_mask))\n",
    "        # q 传的是decoder的\n",
    "        # src_mask作用是去掉padding\n",
    "        return self.residuals[2](y, self.feed_forward)\n",
    "\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "    def forward(self, src, src_mask, trg, trg_pad_mask):\n",
    "        return self.decoder(self.encoder(src, src_mask), src_mask, trg, trg_pad_mask)\n",
    "\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 掩码工作原理示例\n",
    "\n",
    "这是一个针对 Transformer 训练中三种核心掩码（Mask）的详细解析。\n",
    "\n",
    "### 场景设定\n",
    "\n",
    "假设我们正在训练一个翻译模型：**[英文] $\\rightarrow$ [中文]**，且批次最大长度为 5。\n",
    "\n",
    "| Sequence | 原始长度 | 填充后序列 (IDs) |\n",
    "| :--- | :--- | :--- |\n",
    "| **SRC** (英文) | 3 | $s_1, s_2, s_3, \\text{PAD}, \\text{PAD}$ |\n",
    "| **TRG** (中文) | 4 | $t_1, t_2, t_3, t_4, \\text{PAD}$ |\n",
    "\n",
    "我们关注 TRG 序列的**第一个位置 $t_1$** 在解码器中计算注意力时的状态。\n",
    "\n",
    "---\n",
    "\n",
    "### 1. `src_mask` (源序列填充掩码)\n",
    "\n",
    "* **作用位置：** 解码器的**交叉注意力**层（Query $\\in$ TRG, Key/Value $\\in$ SRC）。\n",
    "* **创建依据：** SRC 序列中的 `<pad>` 位置。\n",
    "* **掩码形状 (1x5)：** `[False, False, False, True, True]`\n",
    "* **原理：** 当 $t_1$ 查询 SRC 信息时，`src_mask` 强制它对 $s_4, s_5$ (PAD) 的注意力权重为 **0**。\n",
    "\n",
    "### 2. `trg_pad_mask` (目标序列填充掩码)\n",
    "\n",
    "* **作用位置：** 解码器的**自注意力**层。\n",
    "* **创建依据：** TRG 序列中的 `<pad>` 位置。\n",
    "* **掩码形状 (1x5)：** `[False, False, False, False, True]`\n",
    "* **原理：** 确保 TRG 序列中的任何词（包括 $t_1$）都不会对 TRG 序列末尾的 $\\text{PAD}$ 产生注意力。\n",
    "\n",
    "### 3. `future_mask` (前瞻掩码)\n",
    "\n",
    "* **作用位置：** 解码器的**自注意力**层。\n",
    "* **创建依据：** 序列的几何结构（上三角矩阵）。与 PAD 无关。\n",
    "* **原理：** 强制每个位置只能关注它自己和它之前的词，维护自回归特性。\n",
    "* **$t_1$ 所在行（Query $\\in t_1$）的视图：**\n",
    "    \n",
    "| Q $\\downarrow$ / K $\\rightarrow$ | $t_1$ | $t_2$ | $t_3$ | $t_4$ | $\\text{PAD}$ |\n",
    "| :---: | :---: | :---: | :---: | :---: | :---: |\n",
    "| **$t_1$** | **F** | **T** | **T** | **T** | **T** |\n",
    "\n",
    "---\n",
    "\n",
    "### 4. 联合掩码 (`trg_mask`)\n",
    "\n",
    "**计算方式：** $\\text{trg\\_mask} = \\text{trg\\_pad\\_mask} \\lor \\text{future\\_mask}$ (逻辑或)\n",
    "\n",
    "**$t_1$ 的最终状态：**\n",
    "\n",
    "1.  `future_mask` 已经将 $t_2$ 到 $\\text{PAD}$ 的所有位置标记为 `True`（屏蔽）。\n",
    "2.  `trg_pad_mask` 仅将 $\\text{PAD}$ 标记为 `True`。\n",
    "\n",
    "由于逻辑或运算，最终 $t_1$ 行的注意力矩阵中，**只有 $t_1$ 自身位置**被标记为 `False`（允许关注），其他所有位置都被标记为 `True`（屏蔽）。这确保了 $t_1$ 的输出预测**只基于其自身信息**（以及 $\\text{<sos>}$ 标记），符合严格的自回归约束。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
